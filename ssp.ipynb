{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Propagation (SSP) for Graph Neural Networks\n",
    "This notebook implements the Semi-Supervised Propagation (SSP) approach for Graph Neural Networks as described in the ssp paper. The implementation includes:\n",
    "\n",
    "### Key Components\n",
    "1. Modified GCN Layer: Enhanced Graph Convolutional Network layer with proper normalization and caching\n",
    "2. K-FAC Optimizer: Kronecker-factored approximate curvature optimization for efficient training\n",
    "3. SSP Architecture: Complete model combining convolutional representation learning with semi-supervised propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch_scatter import scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is just bug fix from PR\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_sparse import SparseTensor, matmul, fill_diag, sum, mul_\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "import math\n",
    "\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "@torch.jit._overload\n",
    "def gcn_norm(edge_index, edge_weight=None, num_nodes=None, improved=False,\n",
    "             add_self_loops=True, dtype=None):\n",
    "    # type: (Tensor, OptTensor, Optional[int], bool, bool, Optional[int]) -> PairTensor  # noqa\n",
    "    pass\n",
    "@torch.jit._overload\n",
    "def gcn_norm(edge_index, edge_weight=None, num_nodes=None, improved=False,\n",
    "             add_self_loops=True, dtype=None):\n",
    "    # type: (SparseTensor, OptTensor, Optional[int], bool, bool, Optional[int]) -> SparseTensor  # noqa\n",
    "    pass\n",
    "def gcn_norm(edge_index, edge_weight=None, num_nodes=None, improved=False,\n",
    "             add_self_loops=True, dtype=None):\n",
    "    fill_value = 2. if improved else 1.\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        adj_t = edge_index\n",
    "        if not adj_t.has_value():\n",
    "            adj_t.fill_value(1., dtype=dtype)\n",
    "        if add_self_loops:\n",
    "            adj_t = fill_diag(adj_t, fill_value)\n",
    "        deg = sum(adj_t, dim=1)\n",
    "        deg_inv_sqrt = deg.pow_(-0.5)\n",
    "        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0.)\n",
    "        adj_t = mul_(adj_t, deg_inv_sqrt.view(-1, 1))\n",
    "        adj_t = mul_(adj_t, deg_inv_sqrt.view(1, -1))\n",
    "        return adj_t\n",
    "    else:\n",
    "        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype,\n",
    "                                     device=edge_index.device)\n",
    "        if add_self_loops:\n",
    "            edge_index, tmp_edge_weight = add_remaining_self_loops(\n",
    "                edge_index, edge_weight, fill_value, num_nodes)\n",
    "            assert tmp_edge_weight is not None\n",
    "            edge_weight = tmp_edge_weight\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "        deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow_(-0.5)\n",
    "        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "class GCNConvFix(MessagePassing):\n",
    "    r\"\"\"The graph convolutional operator from the `\"Semi-supervised\n",
    "    Classification with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1609.02907>`_ paper\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "        \\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta},\n",
    "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
    "    adjacency matrix with inserted self-loops and\n",
    "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        improved (bool, optional): If set to :obj:`True`, the layer computes\n",
    "            :math:`\\mathbf{\\hat{A}}` as :math:`\\mathbf{A} + 2\\mathbf{I}`.\n",
    "            (default: :obj:`False`)\n",
    "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
    "            the computation of :math:`\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "            \\mathbf{\\hat{D}}^{-1/2}` on first execution, and will use the\n",
    "            cached version for further executions.\n",
    "            This parameter should only be set to :obj:`True` in transductive\n",
    "            learning scenarios. (default: :obj:`False`)\n",
    "        normalize (bool, optional): Whether to add self-loops and apply\n",
    "            symmetric normalization. (default: :obj:`True`)\n",
    "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\n",
    "            self-loops to the input graph. (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "\n",
    "    _cached_edge_index: Optional[Tuple[torch.Tensor, torch.Tensor]]\n",
    "    _cached_edge_index: Optional[Tuple[Tensor, Tensor]]\n",
    "    _cached_adj_t: Optional[SparseTensor]\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int,\n",
    "                 improved: bool = False, cached: bool = False,\n",
    "                 add_self_loops: bool = True, normalize: bool = True,\n",
    "                 bias: bool = True, **kwargs):\n",
    "        super(GCNConvFix, self).__init__(aggr='add', **kwargs)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.normalize = normalize\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self._cached_edge_index = None\n",
    "        self._cached_adj_t = None\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "        self._cached_edge_index = None\n",
    "        self._cached_adj_t = None\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_weight: OptTensor = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        if self.normalize:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                cache = self._cached_edge_index\n",
    "                if cache is None:\n",
    "                    edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
    "                        edge_index, edge_weight, x.size(self.node_dim),\n",
    "                        self.improved, self.add_self_loops, dtype=x.dtype)\n",
    "                    if self.cached:\n",
    "                        self._cached_edge_index = (edge_index, edge_weight)\n",
    "                else:\n",
    "                    edge_index, edge_weight = cache[0], cache[1]\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                cache = self._cached_adj_t\n",
    "                if cache is None:\n",
    "                    edge_index = gcn_norm(  # yapf: disable\n",
    "                        edge_index, edge_weight, x.size(self.node_dim),\n",
    "                        self.improved, self.add_self_loops, dtype=x.dtype)\n",
    "                    if self.cached:\n",
    "                        self._cached_adj_t = edge_index\n",
    "                else:\n",
    "                    edge_index = cache\n",
    "        x = torch.matmul(x, self.weight)\n",
    "        # propagate_type: (x: Tensor, edge_weight: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n",
    "                             size=None)\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "        return out\n",
    "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
    "        return edge_weight.view(-1, 1) * x_j\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-FAC (Kronecker-Factored Approximate Curvature) Optimizer\n",
    "This implements the K-FAC optimization algorithm adapted for graph neural networks. Key features:\n",
    "- Maintains running estimates of the Fisher Information Matrix\n",
    "- Uses Kronecker-factored approximation for efficient computation\n",
    "- Supports SUA (Spatially Uncorrelated Activations) approximation\n",
    "- Includes options for Tikhonov regularization and pi correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFAC(Optimizer):\n",
    "    #source: https://github.com/russellizadi/ssp/blob/master/experiments/psgd.py\n",
    "    def __init__(self, net, eps, sua=False, pi=False, update_freq=1,\n",
    "                 alpha=1.0, constraint_norm=False):\n",
    "        \"\"\" K-FAC Preconditionner for Linear and Conv2d layers.\n",
    "        Computes the K-FAC of the second moment of the gradients.\n",
    "        It works for Linear and Conv2d layers and silently skip other layers.\n",
    "        Args:\n",
    "            net (torch.nn.Module): Network to precondition.\n",
    "            eps (float): Tikhonov regularization parameter for the inverses.\n",
    "            sua (bool): Applies SUA approximation.\n",
    "            pi (bool): Computes pi correction for Tikhonov regularization.\n",
    "            update_freq (int): Perform inverses every update_freq updates.\n",
    "            alpha (float): Running average parameter (if == 1, no r. ave.).\n",
    "            constraint_norm (bool): Scale the gradients by the squared\n",
    "                fisher norm.\n",
    "        \"\"\"\n",
    "\n",
    "        self.eps = eps\n",
    "        self.sua = sua\n",
    "        self.pi = pi\n",
    "        self.update_freq = update_freq\n",
    "        self.alpha = alpha\n",
    "        self.constraint_norm = constraint_norm\n",
    "        self.params = []\n",
    "        self._fwd_handles = []\n",
    "        self._bwd_handles = []\n",
    "        self._iteration_counter = 0\n",
    "        \n",
    "        for mod in net.modules():\n",
    "            mod_name = mod.__class__.__name__\n",
    "            if mod_name in ['CRD', 'CLS']:\n",
    "                handle = mod.register_forward_pre_hook(self._save_input)\n",
    "                self._fwd_handles.append(handle)\n",
    "                \n",
    "                for sub_mod in mod.modules():\n",
    "                    i_sub_mod = 0\n",
    "                    if hasattr(sub_mod, 'weight'):\n",
    "                        assert i_sub_mod == 0\n",
    "                        handle = sub_mod.register_backward_hook(self._save_grad_output)\n",
    "                        self._bwd_handles.append(handle)\n",
    "                        \n",
    "                        params = [sub_mod.weight]\n",
    "                        if sub_mod.bias is not None:\n",
    "                            params.append(sub_mod.bias)\n",
    "\n",
    "                        d = {'params': params, 'mod': mod, 'sub_mod': sub_mod}\n",
    "                        self.params.append(d)\n",
    "                        i_sub_mod += 1\n",
    "\n",
    "        super(KFAC, self).__init__(self.params, {})\n",
    "\n",
    "    def step(self, update_stats=True, update_params=True, lam=0.):\n",
    "        \"\"\"Performs one step of preconditioning.\"\"\"\n",
    "        self.lam = lam\n",
    "        fisher_norm = 0.\n",
    "        for group in self.param_groups:\n",
    "            \n",
    "            if len(group['params']) == 2:\n",
    "                weight, bias = group['params']\n",
    "            else:\n",
    "                weight = group['params'][0]\n",
    "                bias = None\n",
    "            state = self.state[weight]\n",
    "\n",
    "            # Update convariances and inverses\n",
    "            if update_stats:\n",
    "                if self._iteration_counter % self.update_freq == 0:\n",
    "                    self._compute_covs(group, state)\n",
    "                    ixxt, iggt = self._inv_covs(state['xxt'], state['ggt'],\n",
    "                                                state['num_locations'])\n",
    "                    state['ixxt'] = ixxt\n",
    "                    state['iggt'] = iggt\n",
    "                else:\n",
    "                    if self.alpha != 1:\n",
    "                        self._compute_covs(group, state)\n",
    "\n",
    "            if update_params:\n",
    "                gw, gb = self._precond(weight, bias, group, state)\n",
    "\n",
    "                # Updating gradients\n",
    "                if self.constraint_norm:\n",
    "                    fisher_norm += (weight.grad * gw).sum()\n",
    "\n",
    "                weight.grad.data = gw\n",
    "                if bias is not None:\n",
    "                    if self.constraint_norm:\n",
    "                        fisher_norm += (bias.grad * gb).sum()\n",
    "                    bias.grad.data = gb\n",
    "                    \n",
    "            # Cleaning\n",
    "            if 'x' in self.state[group['mod']]:\n",
    "                del self.state[group['mod']]['x']\n",
    "            if 'gy' in self.state[group['mod']]:\n",
    "                del self.state[group['mod']]['gy']\n",
    "        \n",
    "        # Eventually scale the norm of the gradients\n",
    "        if update_params and self.constraint_norm:\n",
    "            scale = (1. / fisher_norm) ** 0.5\n",
    "            for group in self.param_groups:\n",
    "                for param in group['params']:\n",
    "                    print(param.shape, param)\n",
    "                    param.grad.data *= scale\n",
    "\n",
    "        if update_stats:\n",
    "            self._iteration_counter += 1\n",
    "\n",
    "    def _save_input(self, mod, i):\n",
    "        \"\"\"Saves input of layer to compute covariance.\"\"\"\n",
    "        # i = (x, edge_index)\n",
    "        if mod.training:\n",
    "            self.state[mod]['x'] = i[0]\n",
    "            \n",
    "            self.mask = i[-1]\n",
    "            \n",
    "    def _save_grad_output(self, mod, grad_input, grad_output):\n",
    "        \"\"\"Saves grad on output of layer to compute covariance.\"\"\"\n",
    "        if mod.training:\n",
    "            self.state[mod]['gy'] = grad_output[0] * grad_output[0].size(1)\n",
    "            self._cached_edge_index = mod._cached_edge_index\n",
    "\n",
    "    def _precond(self, weight, bias, group, state):\n",
    "        \"\"\"Applies preconditioning.\"\"\"\n",
    "        ixxt = state['ixxt'] # [d_in x d_in]\n",
    "        iggt = state['iggt'] # [d_out x d_out]\n",
    "        g = weight.grad.data # [d_in x d_out]\n",
    "        s = g.shape\n",
    "\n",
    "        g = g.contiguous().view(-1, g.shape[-1])\n",
    "            \n",
    "        if bias is not None:\n",
    "            gb = bias.grad.data\n",
    "            g = torch.cat([g, gb.view(1, gb.shape[0])], dim=0)\n",
    "\n",
    "        g = torch.mm(ixxt, torch.mm(g, iggt))\n",
    "        if bias is not None:\n",
    "            gb = g[-1].contiguous().view(*bias.shape)\n",
    "            g = g[:-1]\n",
    "        else:\n",
    "            gb = None\n",
    "        g = g.contiguous().view(*s)\n",
    "        return g, gb\n",
    "\n",
    "    def _compute_covs(self, group, state):\n",
    "        \"\"\"Computes the covariances.\"\"\"\n",
    "        sub_mod = group['sub_mod']\n",
    "        x = self.state[group['mod']]['x'] # [n x d_in]\n",
    "        gy = self.state[group['sub_mod']]['gy'] # [n x d_out]\n",
    "        edge_index, edge_weight = self._cached_edge_index # [2, n_edges], [n_edges]\n",
    "        \n",
    "        n = float(self.mask.sum() + self.lam*((~self.mask).sum()))\n",
    "\n",
    "        x = scatter(x[edge_index[0]]*edge_weight[:, None], edge_index[1], dim=0)\n",
    "        \n",
    "        x = x.data.t()\n",
    "\n",
    "        if sub_mod.weight.ndim == 3:\n",
    "            x = x.repeat(sub_mod.weight.shape[0], 1)\n",
    "        \n",
    "\n",
    "\n",
    "        if sub_mod.bias is not None:\n",
    "            ones = torch.ones_like(x[:1])\n",
    "            x = torch.cat([x, ones], dim=0)\n",
    "\n",
    "        if self._iteration_counter == 0:\n",
    "            state['xxt'] = torch.mm(x, x.t()) / n\n",
    "        else:\n",
    "            state['xxt'].addmm_(mat1=x, mat2=x.t(),\n",
    "                                beta=(1. - self.alpha),\n",
    "                                alpha=self.alpha / n)\n",
    "        \n",
    "        gy = gy.data.t() # [d_out x n]\n",
    "\n",
    "        state['num_locations'] = 1\n",
    "        if self._iteration_counter == 0:\n",
    "            state['ggt'] = torch.mm(gy, gy.t()) / n \n",
    "        else:\n",
    "            state['ggt'].addmm_(mat1=gy, mat2=gy.t(),\n",
    "                                beta=(1. - self.alpha),\n",
    "                                alpha=self.alpha / n)\n",
    "\n",
    "    def _inv_covs(self, xxt, ggt, num_locations):\n",
    "        \"\"\"Inverses the covariances.\"\"\"\n",
    "        # Computes pi\n",
    "        pi = 1.0\n",
    "        if self.pi:\n",
    "            tx = torch.trace(xxt) * ggt.shape[0]\n",
    "            tg = torch.trace(ggt) * xxt.shape[0]\n",
    "            pi = (tx / tg)\n",
    "        # Regularizes and inverse\n",
    "        eps = self.eps / num_locations\n",
    "        diag_xxt = xxt.new(xxt.shape[0]).fill_((eps * pi) ** 0.5)\n",
    "        diag_ggt = ggt.new(ggt.shape[0]).fill_((eps / pi) ** 0.5)\n",
    "        ixxt = (xxt + torch.diag(diag_xxt)).inverse()\n",
    "        iggt = (ggt + torch.diag(diag_ggt)).inverse()\n",
    "\n",
    "        return ixxt, iggt\n",
    "\n",
    "    def __del__(self):\n",
    "        for handle in self._fwd_handles + self._bwd_handles:\n",
    "            handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Propagation (SSP) Model Architecture\n",
    "The model consists of three main components:\n",
    "1. CRD (Convolutional Representation Decoder): Initial GCN layer with ReLU and dropout\n",
    "2. CLS (Classification Layer): Final GCN layer with softmax for classification\n",
    "3. SSP: Combines CRD and CLS into a complete semi-supervised learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channels = 16\n",
    "dropout_rate = 0.5\n",
    "\n",
    "\n",
    "class CRD(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out, p):\n",
    "        super(CRD, self).__init__()\n",
    "        self.conv = GCNConvFix(d_in, d_out, cached=True)  # Note: cached=True\n",
    "        self.p = p\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, mask=None):  # Added mask parameter\n",
    "        x = F.relu(self.conv(x, edge_index))\n",
    "        x = F.dropout(x, p=self.p, training=self.training)\n",
    "        return x\n",
    "\n",
    "class CLS(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(CLS, self).__init__()\n",
    "        self.conv = GCNConvFix(d_in, d_out, cached=True)  # Note: cached=True\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv.reset_parameters()\n",
    "    \n",
    "    def forward(self, x, edge_index, mask=None):  # Added mask parameter\n",
    "        x = self.conv(x, edge_index)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "class SSP(torch.nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(SSP, self).__init__()\n",
    "        self.crd = CRD(dataset.num_features, hidden_channels, dropout_rate)\n",
    "        self.cls = CLS(hidden_channels, dataset.num_classes)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.crd.reset_parameters()\n",
    "        self.cls.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.crd(x, edge_index)\n",
    "        x = self.cls(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train(model, optimizer, data, preconditioner=None, lam=0.):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    label = out.max(1)[1]\n",
    "    label[data.train_mask] = data.y[data.train_mask]\n",
    "    label.requires_grad = False\n",
    "    \n",
    "    loss = F.nll_loss(out[data.train_mask], label[data.train_mask])\n",
    "    loss += lam * F.nll_loss(out[~data.train_mask], label[~data.train_mask])\n",
    "    \n",
    "    loss.backward(retain_graph=True)\n",
    "    if preconditioner:\n",
    "        preconditioner.step(lam=lam)\n",
    "    optimizer.step()\n",
    "\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(data)\n",
    "\n",
    "    outs = {}\n",
    "    for key in ['train', 'val', 'test']:\n",
    "        mask = data['{}_mask'.format(key)]\n",
    "        loss = F.nll_loss(logits[mask], data.y[mask]).item()\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "\n",
    "        outs['{} loss'.format(key)] = loss\n",
    "        outs['{} acc'.format(key)] = acc\n",
    "\n",
    "    return outs\n",
    "\n",
    "def run_training(\n",
    "    dataset, \n",
    "    model,\n",
    "    optimizer_name='Adam',\n",
    "    preconditioner_name=None,\n",
    "    runs=10,\n",
    "    epochs=200,\n",
    "    lr=0.01,\n",
    "    weight_decay=5e-4,\n",
    "    early_stopping=10,\n",
    "    momentum=0.9,\n",
    "    eps=0.01,\n",
    "    update_freq=1,\n",
    "    gamma=None,\n",
    "    alpha=None\n",
    "    ):\n",
    "    \n",
    "    val_losses, accs, durations = [], [], []\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    for i_run in range(runs):\n",
    "        data = dataset[0]\n",
    "        data = data.to(device)\n",
    "\n",
    "        model.to(device).reset_parameters()\n",
    "        \n",
    "        if preconditioner_name == 'KFAC':\n",
    "            preconditioner = KFAC(\n",
    "                model, \n",
    "                eps, \n",
    "                sua=False, \n",
    "                pi=False, \n",
    "                update_freq=update_freq,\n",
    "                alpha=alpha if alpha is not None else 1.,\n",
    "                constraint_norm=False\n",
    "            )\n",
    "        else: \n",
    "            preconditioner = None\n",
    "\n",
    "        if optimizer_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(\n",
    "                model.parameters(), \n",
    "                lr=lr, \n",
    "                weight_decay=weight_decay\n",
    "            )\n",
    "        elif optimizer_name == 'SGD':\n",
    "            optimizer = torch.optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=lr, \n",
    "                momentum=momentum,\n",
    "            )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_start = time.perf_counter()\n",
    "        best_val_loss = float('inf')\n",
    "        test_acc = 0\n",
    "        val_loss_history = []\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            lam = (float(epoch)/float(epochs))**gamma if gamma is not None else 0.\n",
    "            train(model, optimizer, data, preconditioner, lam)\n",
    "            eval_info = evaluate(model, data)\n",
    "            \n",
    "            if eval_info['val loss'] < best_val_loss:\n",
    "                best_val_loss = eval_info['val loss']\n",
    "                test_acc = eval_info['test acc']\n",
    "\n",
    "            val_loss_history.append(eval_info['val loss'])\n",
    "            if early_stopping > 0 and epoch > epochs // 2:\n",
    "                tmp = tensor(val_loss_history[-(early_stopping + 1):-1])\n",
    "                if eval_info['val loss'] > tmp.mean().item():\n",
    "                    break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_end = time.perf_counter()\n",
    "        val_losses.append(best_val_loss)\n",
    "        accs.append(test_acc)\n",
    "        durations.append(t_end - t_start)\n",
    "    \n",
    "    loss, acc, duration = tensor(val_losses), tensor(accs), tensor(durations)\n",
    "    print('Val Loss: {:.4f}, Test Accuracy: {:.2f} ± {:.2f}, Duration: {:.3f}'.format(\n",
    "        loss.mean().item(),\n",
    "        100 * acc.mean().item(),\n",
    "        100 * acc.std().item(),\n",
    "        duration.mean().item()\n",
    "    ))\n",
    "    return loss.mean().item(), acc.mean().item(), acc.std().item()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "def get_planetoid_dataset(name, normalize_features=True, split=\"public\"):\n",
    "    path = '/tmp/' + name\n",
    "    if split == \"complete\":\n",
    "        dataset = Planetoid(path, name)\n",
    "        dataset[0].train_mask.fill_(False)\n",
    "        dataset[0].train_mask[:dataset[0].num_nodes - 1000] = 1\n",
    "        dataset[0].val_mask.fill_(False)\n",
    "        dataset[0].val_mask[dataset[0].num_nodes - 1000:dataset[0].num_nodes - 500] = 1\n",
    "        dataset[0].test_mask.fill_(False)\n",
    "        dataset[0].test_mask[dataset[0].num_nodes - 500:] = 1\n",
    "    else:\n",
    "        dataset = Planetoid(path, name, split=split)\n",
    "    if normalize_features:\n",
    "        dataset.transform = NormalizeFeatures()\n",
    "    return dataset\n",
    "\n",
    "dataset = get_planetoid_dataset('Cora', normalize_features=True, split=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/udmurtpsycho/miniconda3/envs/dkr/lib/python3.10/site-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5363, Test Accuracy: 87.05 ± 0.36, Duration: 1.851\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = SSP(dataset)\n",
    "kwargs = {\n",
    "    'dataset': dataset, \n",
    "    'model': SSP(dataset), \n",
    "    'optimizer_name': 'Adam', \n",
    "    'preconditioner_name': 'KFAC', \n",
    "    'runs': 10, \n",
    "    'epochs': 200, \n",
    "    'lr': 0.01, \n",
    "    'weight_decay': 5e-4,\n",
    "    'early_stopping': 0,\n",
    "    'momentum': 0.9,\n",
    "    'eps': 0.46415,\n",
    "    'update_freq': 50,\n",
    "    'gamma': None,\n",
    "    'alpha': None\n",
    "}\n",
    "loss, acc_mean, acc_std = run_training(**kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dkr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
